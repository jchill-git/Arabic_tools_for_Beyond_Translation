{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in sentence alignment file\n",
    "lines=pd.read_excel('Omar_bin_Seid_alignment.xlsx','Logical_Breaks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set punctuation, augmented to include unique punctuation\n",
    "punct_list=string.punctuation+'âˆµ'\n",
    "\n",
    "#create table with core values for Perseus token system\n",
    "df = pd.DataFrame(columns = ['value', 'punctuation', 'space_after', 'position', 've_ref','idx'])\n",
    "\n",
    "#CAMel Labs parser and morphological tokenizer intialization\n",
    "\n",
    "#[CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing]\n",
    "#(https://aclanthology.org/2020.lrec-1.868) (Obeid et al., LREC 2020)\n",
    "\n",
    "mle_msa = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "msa_atb_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, scheme='atbtok')\n",
    "\n",
    "#part trackers\n",
    "chunk = 0\n",
    "sent = 0\n",
    "idx=0\n",
    "\n",
    "#assumes section and page breaks marked in excel file and skips those markers\n",
    "for sent_id, line in enumerate(lines['Arabic']):\n",
    "    if(re.search('PAGE', line) is not None or line == 'SECTION BREAK'):\n",
    "        chunk += 1\n",
    "        sent = 1\n",
    "    else:\n",
    "        #tokenize Arabic line\n",
    "        simp_tok= simple_word_tokenize(line)\n",
    "        morph_tok = msa_atb_tokenizer.tokenize(simp_tok)\n",
    "        \n",
    "        #subtoken handling, seperating attached subtokens from CAMel tools tokens\n",
    "        tokens=list()\n",
    "        for token in morph_tok: \n",
    "            if (re.search('_+', token) is not None):\n",
    "                \n",
    "                sub_tokens_first=token.split('+_')\n",
    "                sub_tokens_second=list()\n",
    "                for sub_token in sub_tokens_first:\n",
    "                    sub_tokens_second.append(sub_token.split('_+'))\n",
    "                sub_tokens=flatten(sub_tokens_second)\n",
    "                \n",
    "                #handling need for space indictators between subword tokens\n",
    "                for i in range(0,len(sub_tokens)):        \n",
    "                    if i == len(sub_tokens)-1:\n",
    "                        tokens.append(sub_tokens[i])\n",
    "                    else:\n",
    "                        tokens.append(sub_tokens[i]+\"*\")\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "        \n",
    "        #mark punctuation tokens in table\n",
    "        for position, token in enumerate(tokens):\n",
    "            #punctuation\n",
    "            punct=''\n",
    "            if token in punct_list:\n",
    "                punct='y'\n",
    "                df.loc[idx-1, ['space_after']] = ['n']\n",
    "            else:\n",
    "                punct=''\n",
    "                \n",
    "            #leading subtoken in joint token\n",
    "            if \"*\" in token:\n",
    "                space='n'\n",
    "                token=token.replace('*', '')\n",
    "            else:\n",
    "                space=''\n",
    "            \n",
    "            #add to table\n",
    "            df = df.append({'value' : token, 'punctuation' : punct, 'space_after' : space, 'position': position+1, 've_ref': str(chunk)+'.'+str(sent)+'.t'+str(position+1), 'idx': idx},\n",
    "                ignore_index = True)\n",
    "            idx+=1\n",
    "        sent += 1\n",
    "#save token table to csv\n",
    "df.to_csv('bin_said_tokens_ar.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
